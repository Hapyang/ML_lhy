import pandas as pd
import numpy as np
from matplotlib import pyplot as plt


def loadDataSet():
    dataRaw = pd.read_csv('D:\DL_lhy\HW01\\trainN.csv', index_col=0)
    piece = dataRaw[0:18]
    data = piece.T

    for i in range(1, 240):
        piece = dataRaw[18 * i:18 * i + 18]
        pieceT = piece.T
        data = pd.merge(data, pieceT, how='outer')
    data['bias'] = 1
    data.iloc[:, 10] = 0  ## 0  represents no rain

    return data


def gradientDescent(trainData, weights0, alpha, num_iters):
    Loss = []
    for k in range(num_iters):
        n = weights0.size
        weights = weights0
        for j in range(n):
            if j == 9:  # [9] is pm2.5
                continue
            S = 0
            for i in range(trainData.shape[0]):
                data_single = trainData.iloc[i]
                S = S + (data_single.dot(weights) - data_single[9]) * data_single[j]
            weights[j] = weights[j] - S * alpha / np.sqrt(0.01*k + 1) / trainData.shape[0]

        Loss_k = 0
        for i in range(trainData.shape[0]):
            data_single = trainData.iloc[i]
            Loss_k = Loss_k + np.absolute(data_single.dot(weights) - data_single[9])
        Loss_k = Loss_k / 2 / trainData.shape[0]  # loss = (y^-(b+wx)).^2/2m
        Loss.append(Loss_k)
        print("the round completed", k, "the loss is", Loss_k, "\n")
    return weights, Loss


def stochasticGradientDescent(trainData, weights0, alpha, num_iters):
    Loss = []
    for k in range(num_iters):
        n = weights0.size
        weights = weights0

        j = 0
        for i in range(trainData.shape[0]):
            if j == 9:
                j = j + 1
                continue
            data_single = trainData.iloc[i]
            S = (data_single.dot(weights) - data_single[9]) * data_single[j]
            weights[j] = weights[j] - S * alpha / np.sqrt(0.1*k + 1)
            j = j + 1
            if j == 19:
                j = 0

        Loss_k = 0
        for i in range(trainData.shape[0]):
            data_single = trainData.iloc[i]
            Loss_k = Loss_k + np.absolute(data_single.dot(weights) - data_single[9])
        Loss_k = Loss_k / 2 / trainData.shape[0]  # loss = (y^-(b+wx)).^2/2m
        Loss.append(Loss_k)
        print("the round completed", k, "the loss is", Loss_k, "\n")

    return weights, Loss


def calLoss_on_testingData(testingData, weights):
    n = testingData.shape[0]
    Loss = 0
    for i in range(n):
        data_single = testingData.iloc[i]
        Loss_k = Loss_k + np.absolute(data_single.dot(weights) - data_single[9])
    Loss = Loss / 2 / n
    return Loss


def feature_scaling(trainData):
    data_feature_scaling = trainData.T
    for i in range(data_feature_scaling.shape[0]):
        if i == 9 or i == 10 or i == 18:
            var_i = data_feature_scaling.iloc[i]
            temp = pd.to_numeric(var_i)
            data_feature_scaling.iloc[i] = temp
            continue
        var_i = data_feature_scaling.iloc[i]
        temp = pd.to_numeric(var_i)
        var_mean = temp.mean()
        var_std = temp.std()
        temp = (temp - var_mean) / var_std  # mean =0 ,std =1
        data_feature_scaling.iloc[i] = temp

    data_feature_scaling = data_feature_scaling.T
    return data_feature_scaling


def main():
    trainData = loadDataSet()
    # feature scaling
    data_feature_scaling = feature_scaling(trainData)
    alpha_gd = 0.5
    alpha_sgd = 0.0002
    num_iters = 1000
    weights = np.random.random(19)  # 18+1
    weights[9] = 0  # [9] is pm2.5
    [weights, Loss] = gradientDescent(data_feature_scaling.iloc[:3000], weights, alpha_gd, num_iters)  # 3000/5640 data used to train
    #[weights, Loss] = stochasticGradientDescent(data_feature_scaling.iloc[:3000], weights, alpha_sgd,num_iters)  # 3000/5640 data used to train
    t = np.arange(0, len(Loss))
    plt.plot(t, Loss)
    plt.show()
    loss_on_TestingData = calLoss_on_testingData(data_feature_scaling[3000:], weights)
    print("the loss of model on testingdata is", loss_on_TestingData)


if __name__ == '__main__':
    main()
